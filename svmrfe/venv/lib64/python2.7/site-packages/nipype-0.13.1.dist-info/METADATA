Metadata-Version: 2.0
Name: nipype
Version: 0.13.1
Summary: Neuroimaging in Python: Pipelines and Interfaces
Home-page: http://nipy.org/nipype
Author: nipype developers
Author-email: neuroimaging@python.org
License: Apache License, 2.0
Download-URL: http://github.com/nipy/nipype/archives/master
Platform: O
Platform: S
Platform: 
Platform: I
Platform: n
Platform: d
Platform: e
Platform: p
Platform: e
Platform: n
Platform: d
Platform: e
Platform: n
Platform: t
Classifier: Development Status :: 5 - Production/Stable
Classifier: Environment :: Console
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Operating System :: MacOS :: MacOS X
Classifier: Operating System :: POSIX :: Linux
Classifier: Programming Language :: Python :: 2.7
Classifier: Programming Language :: Python :: 3.4
Classifier: Programming Language :: Python :: 3.5
Classifier: Programming Language :: Python :: 3.6
Classifier: Topic :: Scientific/Engineering
Provides: nipype
Requires-Dist: click (>=6.6.0)
Requires-Dist: configparser
Requires-Dist: funcsigs
Requires-Dist: future (>=0.16.0)
Requires-Dist: mock
Requires-Dist: networkx (>=1.9)
Requires-Dist: nibabel (>=2.1.0)
Requires-Dist: numpy (>=1.8.2)
Requires-Dist: prov (>=1.5.0)
Requires-Dist: pydotplus
Requires-Dist: pytest (>=3.0)
Requires-Dist: python-dateutil (>=2.2)
Requires-Dist: scipy (>=0.14)
Requires-Dist: simplejson (>=3.8.0)
Requires-Dist: traits (>=4.6)
Provides-Extra: all
Requires-Dist: Sphinx (>=1.4); extra == 'all'
Requires-Dist: codecov; extra == 'all'
Requires-Dist: dipy; extra == 'all'
Requires-Dist: duecredit; extra == 'all'
Requires-Dist: matplotlib; extra == 'all'
Requires-Dist: matplotlib; extra == 'all'
Requires-Dist: nilearn; extra == 'all'
Requires-Dist: nipy; extra == 'all'
Requires-Dist: nitime; extra == 'all'
Requires-Dist: psutil; extra == 'all'
Requires-Dist: pydotplus; extra == 'all'
Requires-Dist: pytest-cov; extra == 'all'
Requires-Dist: xvfbwrapper; extra == 'all'
Provides-Extra: doc
Requires-Dist: Sphinx (>=1.4); extra == 'doc'
Requires-Dist: matplotlib; extra == 'doc'
Requires-Dist: pydotplus; extra == 'doc'
Provides-Extra: duecredit
Requires-Dist: duecredit; extra == 'duecredit'
Provides-Extra: nipy
Requires-Dist: dipy; extra == 'nipy'
Requires-Dist: matplotlib; extra == 'nipy'
Requires-Dist: nilearn; extra == 'nipy'
Requires-Dist: nipy; extra == 'nipy'
Requires-Dist: nitime; extra == 'nipy'
Provides-Extra: profiler
Requires-Dist: psutil; extra == 'profiler'
Provides-Extra: tests
Requires-Dist: codecov; extra == 'tests'
Requires-Dist: pytest-cov; extra == 'tests'
Provides-Extra: xvfbwrapper
Requires-Dist: xvfbwrapper; extra == 'xvfbwrapper'

========================================================
NIPYPE: Neuroimaging in Python: Pipelines and Interfaces
========================================================

Current neuroimaging software offer users an incredible opportunity to analyze data using a variety of different algorithms. However, this has resulted in a heterogeneous collection of specialized applications without transparent interoperability or a uniform operating interface.

*Nipype*, an open-source, community-developed initiative under the umbrella of NiPy_, is a Python project that provides a uniform interface to existing neuroimaging software and facilitates interaction between these packages within a single workflow. Nipype provides an environment that encourages interactive exploration of algorithms from different packages (e.g., AFNI, ANTS, BRAINS, BrainSuite, Camino, FreeSurfer, FSL, MNE, MRtrix, MNE, Nipy, Slicer, SPM), eases the design of workflows within and between packages, and reduces the learning curve necessary to use different packages. Nipype is creating a collaborative platform for neuroimaging software development in a high-level language and addressing limitations of existing pipeline systems.

*Nipype* allows you to:

* easily interact with tools from different software packages
* combine processing steps from different software packages
* develop new workflows faster by reusing common steps from old ones
* process data faster by running it in parallel on many cores/machines
* make your research easily reproducible
* share your processing workflows with the community


